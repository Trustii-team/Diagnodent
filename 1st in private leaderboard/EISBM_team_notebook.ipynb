{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139a4336-0163-4e85-9c13-a3ba78d3a997",
   "metadata": {},
   "source": [
    "# Diagnodent challenge: Diagnosis of oral rare diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fadaa1-fca1-4ca3-8582-784932389273",
   "metadata": {},
   "source": [
    "This notebook is authored by Albert Saporta, Bertrand De Meulder and Johann Pellet, Association EISBM, France."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4d13e-3641-4682-8fcc-7534336f220d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37bdc6-7802-43ff-81f9-88d641e05b02",
   "metadata": {},
   "source": [
    "The notebook is organized as follow:\n",
    "\n",
    "1.1 Structure   \n",
    "1.2 Introduction   \n",
    "1.3 Importing libraries   \n",
    "1.4 Exploratory data analysis and resulting assumptions   \n",
    "1.5 Loading the data for preprocessing   \n",
    "1.6 Data preprocessing  \n",
    "1.7 Data augmentation   \n",
    "1.8 Loading the dataset for modelling   \n",
    "1.9 Modelling parameters   \n",
    "1.10 Training     \n",
    "1.11 Evaluation of the model on the test set   \n",
    "1.12 Model explainability definition   \n",
    "1.13 Submission   \n",
    "1.14 Comments and conclusions   \n",
    "1.15 Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229d71d-168a-41f6-ae5e-2335919d5d6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04d375-a3ea-4942-86d7-ac60783210ea",
   "metadata": {},
   "source": [
    "Amelogenesis Imperfecta (AI) is an extremely rare oral disease that affects the development of teeth enamel, leading to a range of symptoms. \n",
    "The disease is typically diagnosed based on clinical examination, radiographic imaging, and/or biopsy. \n",
    "Accurately diagnosing this disease is crucial for providing appropriate treatment, but it can be challenging due to its rarity and the wide range of symptoms it can cause.\n",
    "\n",
    "Goal of this challenge:\n",
    "- To develop a deep learning model that can assist in the diagnosis of AI by accurately classifying its symptoms based on oral images (photo and radiographies).\n",
    "- The model must output a set of labels indicating the cohort, the  presence or absence of specific symptoms of AI, the gene responsible if there is one and the syndromic nature.\n",
    "- To provide an efficient image pre processing pipeline.\n",
    "\n",
    "Requirements:\n",
    "- The final algorithm is aimed to be used in a clinical setting and therefore has to be of high quality and provide explainability. \n",
    "- Train < 6 hours\n",
    "- Inference for one image on CPU < 1 minute\n",
    "- Provide SHAP values to display explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9b634-d120-4490-802d-7df99f9efdb0",
   "metadata": {},
   "source": [
    "##  Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85e983-cda0-49ff-acb7-464bb4e818dc",
   "metadata": {},
   "source": [
    "We import the necessary libraries to perform the analyses.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98862063-c7ac-49dc-affb-36e01b5845c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.8.0.74)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trustii.io has already installed os, pandas, seaborn, malplotlib, numpy, json, pytorch, sklearn, pickle\n",
    "# We do not know the versions of those lbraries. \n",
    "\n",
    "# We add to this list opencv (opencv-python 4.7.0.72) and grad-cam\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models import efficientnet_v2_l, efficientnet_b7, resnet50\n",
    "\n",
    "# We however leave this line commented to allow users to load other versions of Resnet that we tested during the development challenge. \n",
    "# from torchvision.models import resnet101, ResNet101_Weights,resnet50, ResNet50_Weights,resnet34, ResNet34_Weights,resnet18, ResNet18_Weights\n",
    "\n",
    "from tqdm.notebook  import tqdm\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# library for explainable AI\n",
    "!pip install pytorch-gradcam\n",
    "!pip install grad-cam\n",
    "import pytorch_grad_cam\n",
    "from pytorch_grad_cam import AblationCAM, EigenCAM,GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_accross_batch_and_channels, scale_cam_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de79dc5-bd01-480e-a45b-362f345f4d84",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Exploratory data analysis and resulting assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82114c03-e3d5-4a96-b28c-3aadab53b5a8",
   "metadata": {},
   "source": [
    "In this section we try to get a better understanding of the data and develop a strategy for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f518f9-176a-4fd8-a2a8-c1d605569985",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shape of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391aed49-78b6-4de8-a5b7-f02543576857",
   "metadata": {},
   "source": [
    "We take a first look at the training data.   \n",
    "First we read the train.csv file and look at its dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb71cb9-50fb-4b71-bb34-277fb8284345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path=\"data/train.csv\"\n",
    "train_dataframe=pd.read_csv(train_path)\n",
    "train_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f78b7-641d-48ef-b8ac-afdd46e110f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "The training dataset is a table with 6 columns and 254 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53981b-500c-4982-b24c-99e677cc608c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df=train_dataframe.copy()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabe51a-98a9-4642-a323-8e83f73b58c4",
   "metadata": {},
   "source": [
    "We check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af54db9-d11d-4155-ad3a-e6574538f350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2106b05-2f4e-4880-8340-2d1e01af5d40",
   "metadata": {},
   "source": [
    "No missing values in the dataset. However, as some of the label None in the Responsible_Gene_Name class mean that the genetic test has not been done, the label None should be considered as a missing value in that case. This also means that the label None in the class Is_Isolated_Syndromic mean that we cannot conclude if the case is Isolated or Syndromic, and the label None should also be considered as a missing value in that case.\n",
    "\n",
    "We then look at the Cohort column in more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567a896-8117-4c5d-978b-a5aa15ceac26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['Cohort'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abfec6-22a4-4f4b-977c-4c7b93f8846e",
   "metadata": {},
   "source": [
    "Amelogenesis Imperfecta (AI) and Dentinogenesis Imperfecta (DI) are two hereditary abnormalities of dental hard tissues.\n",
    "\n",
    "In both conditions, the teeth are weak and prone to breakage. Both diseases (AI and DI) can be treated with many similar strategies, including restorative, prosthetic, peridontal, surgical and orthodontic treatments (Sawan, 2021). The origins of the two diseases can be genetic, epigenetic and environmental factors. AI is caused by disturbed development processes, such as mutations of the AMELX gene that encodes secretion of extracellular matrix proteins during the enamel formation. Several other genes have been identified to cause AI, such as ENAM, MMP20, KLK4, FAM83H or FAM20A. DI is caused by mutations in COLIA1, COLIA2 and DSPP (Januarti *et al*, 2023, \n",
    "Simancas-Escorcia *et al*, 2018, Goldberg *et al*, 2019, An *et al*, 2015, Jaureguiberry *et al*, 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa48f73-b470-468c-8b49-793142ff4af0",
   "metadata": {},
   "source": [
    "Let's look in more details at the AI types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a9bee-b353-47cc-be4d-3f40d129f0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['AI_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4543f1-179f-48ec-a0bc-5540e429ada4",
   "metadata": {},
   "source": [
    "We see that there is a large imbalance in the AI types.   \n",
    "Following the Witkop 1988 classification, we see that we have:\n",
    "\n",
    "46 patients of AI type I (hypoplastic),  \n",
    "46 patients of AI type II (hypomature),  \n",
    "16 patients of AI type III (hypocalcification),  \n",
    "7 patients of AI type IV (hypomature/hypoplastic/taurodontism).   \n",
    "\n",
    "This will have implications for setting the model expectations and for the data augmentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49182ce-9a0d-42b5-b375-5ddea1799f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualisation of qualitative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620084d2-bd0a-4fc9-9783-f4484cdd9720",
   "metadata": {},
   "source": [
    "Here we try several ways of plotting the qualitative features to get a better understanding of the categorical data features. \n",
    "\n",
    "- We first look at the relationship between genes and the disease categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cca92-2875-4c6a-8b79-68e601d5e8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(pd.crosstab(train_df['Responsible_Gene_Name'],train_df['AI_Type']),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b099826-31fa-4562-9649-cd92e117f318",
   "metadata": {},
   "source": [
    "This plot is a heatmap of the Responsible_Gene_Names across the 4 AI categories. \n",
    "\n",
    "From this, we see that there are few genes that are sufficiently represented to try to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11962d-f0dd-4d2e-8f41-37a9865620a5",
   "metadata": {},
   "source": [
    "**Therefore, we choose to only predict whether patients have mutations in MMP20, FAM83H, AMELX, ENAM, FAM20A and encode all others as 'None'.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754496e3-0f89-4f68-a1e3-414ce8364146",
   "metadata": {
    "tags": []
   },
   "source": [
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c12b4a-3c4d-4fec-b421-27ad5fdb25ff",
   "metadata": {},
   "source": [
    "- We then look at the Is_Isolated_Syndromic column. This variable has three levels: 'isolated', 'syndromic', or 'none'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a95e60-d37c-44ba-84dc-61e1514f4b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(pd.crosstab(train_df['Cohort'],train_df['Is_Isolated_Syndromic']),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b96ba8-8741-4848-b4e3-e4dd5f27e640",
   "metadata": {},
   "source": [
    "From the literature, we know that both AI and DI are diseases that can exist in isolation, or in association with other symptoms in a syndrome (http://www.rarenet.eu/wp-content/uploads/2016/12/GB-RARENET_1_amelogenesis.pdf, de la Dure-Molla et al, 2015).\n",
    "\n",
    "In the data, we see that all DI patients have an isolated status but that there is no unique relationship between the syndromic or isolated status and the AI subtypes. However, all type IV patients are either ‘isolated’ (N=2) or ‘none’ (N=5). \n",
    "\n",
    "\n",
    "From this low number of patients, we cannot make any inference about the structure of the dataset. \n",
    "\n",
    "**We will therefore consider all levels of this variable for all disease categories.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10224756-9803-4ea1-bc05-d367168215bd",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818af58e-05fa-47ad-91b7-ffb9a7a16dec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Images analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4cfde-b128-4272-a5d7-ec65cf48738d",
   "metadata": {},
   "source": [
    "We then take a first look at the images available. We load the 'images.csv' file and look at its structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bb1d3-a800-4591-8bd7-e22fc8331785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_id_path=\"data/images.csv\"\n",
    "image_id_df=pd.read_csv(image_id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c91c1-273b-4b34-b866-aa3c41785505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f49cd5-9067-4cc3-b744-f4625e018b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_id_list=pd.read_csv(image_id_path)[['Patient_ID','Image_ID','Patient_Age_In_Image','Image_Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b898516-3bc7-43b3-aff3-723692fcb94f",
   "metadata": {},
   "source": [
    "We see that for each patient, we have different numbers of images, a mix of panoramic radiographies and colour photos at different ages and dentition cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1566a35-bdca-46b9-8000-6cc881accb30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_id_counts=image_id_df.groupby(['Patient_ID','Image_ID']).value_counts()\n",
    "image_id_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc5a71-b7f3-4d70-b3dd-387cfc696547",
   "metadata": {},
   "source": [
    "For technical reasons, we will have to harmonize the size of the images and transform the black and white radiographies into artificial colour images by assigning the black and white value to one colour channel and repeat the value to the other two primary colour channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f6ed7-75e9-4600-94e4-0dd1e6bfafc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the data for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17f7d9-d40a-4fc2-b424-1bda82f42957",
   "metadata": {},
   "source": [
    "- We load the csv file containing the patient list and targets in the dataframe **patient_labels_list**.\n",
    "- We load the csv file containing the patients' images list in the dataframe **image_id_list**.\n",
    "- We create the dataframe **targets** that contains the targets we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399adbbd-962b-414d-9f7b-4409ae40280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\"data/train.csv\"\n",
    "image_id_path=\"data/images.csv\"\n",
    "patient_labels_list=pd.read_csv(train_path)[['Patient_ID','Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]\n",
    "image_id_list=pd.read_csv(image_id_path)[['Patient_ID','Image_ID','Patient_Age_In_Image','Image_Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c870f4-697c-42d9-8df2-e52b0633e938",
   "metadata": {},
   "source": [
    "##  Data preprocessing\n",
    "- As explained in the previous section, we choose to only predict whether patients have mutations in MMP20, FAM83H, AMELX, FAM20A and ENAM and set all others to 'None'.\n",
    "- We encode the targets.\n",
    "- We normalize and resize the images.\n",
    "- We increase the training dataset size with augmentations on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260404ff-31b2-4d3a-8417-7878c8009d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_labels_list=pd.read_csv(train_path)[['Patient_ID','Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]\n",
    "genes_to_remove=['ACP4','AMBN',   'CNNM4', 'COL17A1','COL7A1', 'DLX3', 'GALNS',  'KLK4', 'LAMA3', 'LAMB2', 'LAMB3',\n",
    "                 'LAMC2', 'LTBP3', 'ODAPH', 'ROGDI', 'SLC10A7', 'SLC13A5', 'SLC24A4','WDR72']\n",
    "for gene in genes_to_remove:\n",
    "    patient_labels_list['Responsible_Gene_Name'] = patient_labels_list['Responsible_Gene_Name'].replace(gene,'None')\n",
    "patient_labels_list['Responsible_Gene_Name'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a215d4-618b-49cc-854a-75c6b7a25954",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=patient_labels_list[['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755d931-0e6d-4698-b025-326054e4272b",
   "metadata": {},
   "source": [
    "### Targets encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3933f-d0bb-4aff-9a49-1500511abd3c",
   "metadata": {},
   "source": [
    "In this section we choose the one-hot encoding scheme, using LabelBinarizer from scikit learn https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html.\n",
    "\n",
    "We create four LabelBinarizer encoders, one per class.   \n",
    "Each encoded class/column is a vector with the length equal to the amount of possible labels in the class, with exactly one position set to 1 and all others set to 0. \n",
    "\n",
    "<img src='Figures/Target encoding.png' width=800 height=200>\n",
    "\n",
    "The classes and their associated labels are as follows: \n",
    "- Cohort: Amelogenesis Imperfecta, Dentinogenesis Imperfecta, Normal,\n",
    "- AI type: AI_Hypomature, AI_Hypoplastic, AI_Hypocalcification/Hypomineralization, AI_Hypomature/AI_Hypoplastic/Taurodontism, None,\n",
    "- Responsible_Gene_Name: MMP20, FAM83H, AMELX, FAM20A, ENAM, None,\n",
    "- Is_Isolated_Syndromic: None, Isolated, Syndromic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79935e9e-01d8-48ec-b7c8-c32b4b6b4dc9",
   "metadata": {},
   "source": [
    "The following function encode the training target as well as the target from the augmentated training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56baa639-518c-408b-9634-172fbbecb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_target_encoding(targets,augmented_targets):\n",
    "    encoded_tar,augmented_encoded_tar,augmented_encoded_rare_tar={},{},{}\n",
    "    for i, key in enumerate(targets):\n",
    "        endoder =LabelBinarizer()\n",
    "        encoded_tar[key]=endoder.fit_transform(targets[key]).tolist()\n",
    "        augmented_encoded_tar[key]=endoder.transform(augmented_targets[key]).tolist()\n",
    "        pickle.dump(endoder ,open(f\"results/{key}_encoder\",\"wb\"))\n",
    "    return encoded_tar, augmented_encoded_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23520a9e-7d1f-4440-90da-e659fc0a2fba",
   "metadata": {},
   "source": [
    "### Image pre processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19941f4e-d1ac-4c60-8168-d75101a96cd8",
   "metadata": {},
   "source": [
    "We will apply several pre processing transformation:\n",
    "\n",
    "- Resizing the image (400*400),\n",
    "- Normalizing the image in the range [0,1] (in the class Dental_disease_Dataset defined below).\n",
    "\n",
    "We choose the size 400*400 after numerous tests. We obtained the best results with this size while keeping the training time below six hours, as per the requirements of the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368db18c-dc6b-4a59-8cd9-fa32fb4d3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=400\n",
    "preprocessing=transforms.Compose([transforms.Resize((image_size,image_size))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff964a-84b7-491c-a615-911ecce8af7d",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b8c62-7810-4a88-aa90-33af90875617",
   "metadata": {},
   "source": [
    "The most important point where machine learning and AI models fail is the lack of relevant and quality data. \n",
    "In order to boost the model's performances, it is common to use data augmentation. Data augmentation is a process by which we generate additional training image data by applying different transformations to existing images. \n",
    "\n",
    "Notice that we evaluated the model's performance with and without augmentation, and adding augmentation reduced overfitting and improved the F1 score on the test set.\n",
    "\n",
    "We will perform augmentation only for the Cohorts AI and DI. One requirement of this challenge is the robustness of the model in real life scenarios where the image quality might change.  For example, different perspective or angle of the mouth, noise or variance in lightning conditions such as contrast light.    \n",
    "Taking these into account, we choose to perform augmentation in several ways: \n",
    "\n",
    "- ColorJitter, in order to  randomly change the brightness, contrast or saturation of the image https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html,\n",
    "- RandomHorizontalFlip with of probability of 50%, in order to randomly flip the image https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196df148-dbcf-48aa-803a-a285f104ca3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "- We define the augmented dataset **augmented_df** as a subset of the training  dataset.\n",
    "- We remove the cohort Normal and keep only the cohorts Amelogeneis Imperfecta and Dentinogenesis Imperfecta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11fb7a-56e6-4951-bb8f-2a5121038274",
   "metadata": {},
   "source": [
    "We use 80% of these augmented images for training, and 20% for validation, and also stratified on the subtypes of AI. It is important to balance the number of augmented images in the validation set. The lowered quality of the augmented images needs to be reflected in both training and validation sets for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3921f-8efb-4245-9d73-88f4b4470a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df=patient_labels_list[patient_labels_list['Cohort']!='Normal']\n",
    "augmented_targets=augmented_df[['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981c153-633c-49ca-b12c-ad2567c314b6",
   "metadata": {},
   "source": [
    "<img src='Figures/Numbers with augmentation.png' width=800 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c90454-d846-416c-9723-35b49e985f9b",
   "metadata": {},
   "source": [
    "Below, we summarize the proportions of data in the training and validation sets.\n",
    "\n",
    "<img src='Figures/Augmentation percentages.png' width=600 height=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73764ea-3c27-4c80-8bc0-e6ea43bd86b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cj=0.2\n",
    "augmented_preprocessing  = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size)),\n",
    "        transforms.ColorJitter(brightness=cj,contrast=cj, saturation=cj),\n",
    "        transforms.RandomHorizontalFlip(p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e5ac2-2750-4dc6-87a7-812a19b2766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tar, augmented_encoded_tar=one_hot_target_encoding(targets,augmented_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf3510-4103-46b8-a12a-7aee8f18d657",
   "metadata": {},
   "source": [
    "- We put the encoded target in the **patient_labels_list** dataframe.\n",
    "- We create a list of training images and a list of the corresponding training target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96dc17d-f59f-4e49-95df-0d74df812dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_targets=pd.DataFrame(encoded_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d232a-fccc-4411-b729-0cc2c5fb7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_labels_list[['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]=encoded_targets\n",
    "patient_labels_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75a0f9-9638-44f6-a5d7-7c1f19d0a028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_image_list,training_targets_list=[],[]\n",
    "pati_id=patient_labels_list['Patient_ID'].tolist()\n",
    "for pat_id in pati_id:\n",
    "    ai_type_not_padded=patient_labels_list[patient_labels_list['Patient_ID']==pat_id][['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']].values.tolist()[0]#to_numpy(dtype=int)#.item()\n",
    "    image_list=image_id_list[image_id_list['Patient_ID']==pat_id]['Image_ID'].tolist()\n",
    "    ai_type_list=[ai_type_not_padded]*len(image_list)\n",
    "\n",
    "    for k in range(len(image_list)):\n",
    "        training_image_list.append(image_list[k])\n",
    "        training_targets_list.append(ai_type_list[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb28da-372c-4698-a30c-82dc0076a318",
   "metadata": {},
   "source": [
    "- We put the encoded augmented target in the **augmented_df** dataframe.\n",
    "- We create a list of augmented images and a list of the corresponding target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f233c30-06e0-4814-a803-387e8e107655",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_encoded_targets=pd.DataFrame(augmented_encoded_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee33121-b2ad-4640-8fb7-1b3cfde505fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "augmented_df=augmented_df.reset_index()\n",
    "augmented_df[['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]=augmented_encoded_targets\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d931f7-39bf-4a30-95c2-8a9a9f7d5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image_list,augmented_targets_list=[],[]\n",
    "pati_id=augmented_df['Patient_ID'].tolist()\n",
    "for pat_id in pati_id:\n",
    "    aug_ai_type=augmented_df[augmented_df['Patient_ID']==pat_id][['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']].values.tolist()[0]#to_numpy(dtype=int)#.item()\n",
    "    aug_image_list=image_id_list[image_id_list['Patient_ID']==pat_id]['Image_ID'].tolist()\n",
    "    aug_ai_type_list=[aug_ai_type]*len(aug_image_list)\n",
    "    for k in range(len(aug_image_list)):\n",
    "        augmented_image_list.append(aug_image_list[k])\n",
    "        augmented_targets_list.append(aug_ai_type_list[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582962ed-0a89-4d27-b51f-73c9304e48ad",
   "metadata": {},
   "source": [
    "## Loading the datatset for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de566e68-735f-4330-9ec8-3c8deb3b0332",
   "metadata": {},
   "source": [
    "### Dataset loader class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497d1b3-f14c-4b5b-b73c-b91a11073dd1",
   "metadata": {},
   "source": [
    "The  class **Dental_disease_Dataset** loads the dataset and takes as input:\n",
    "- The list of raw images (no preprocessing): **training_image_list** or **augmented_image_list**\n",
    "- The list of one hot encoded targets: **training_targets_list** or **augmented_targets_list** \n",
    "- The list of transformations, either for image preprocessing or image augmentation: **preprocessing** or **augmented_preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dee18-8f81-4f56-9585-74f5aa6a722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dental_disease_Dataset(object):\n",
    "    def __init__(self,image_list,labels_list, transformations=None):\n",
    "        self.transforms=transformations\n",
    "        self.imgs = image_list\n",
    "        self.labels = labels_list\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        image, label =  cv2.imread(\"data/images/\"+self.imgs[idx]), self.labels[idx]\n",
    "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "        image = torch.tensor(image).permute(2, 0, 1).float()/255 \n",
    "        if self.transforms!=None:\n",
    "            image=self.transforms(image)\n",
    "\n",
    "        label_cohort,label_AI_type, label_genes, label_syndrome = label[0], label[1], label[2], label[3]\n",
    "        label_dictionary={}\n",
    "        label_dictionary['Cohort']=torch.tensor(label_cohort)\n",
    "        label_dictionary['AI_Type']=torch.tensor(label_AI_type)\n",
    "        label_dictionary['Responsible_Gene_Name']=torch.tensor(label_genes)\n",
    "        label_dictionary['Is_Isolated_Syndromic']=torch.tensor(label_syndrome)\n",
    "  \n",
    "        return image, label_dictionary \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89794b5c-a5a9-4f9c-877f-62ce032d233a",
   "metadata": {},
   "source": [
    "We transform the lists of encoded targets and images to numpy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d2ea9-8743-4c3e-afb5-41accf2a8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array=np.array(training_targets_list,dtype=object)\n",
    "encoded_labels_array=labels_array\n",
    "\n",
    "augmented_labels_array=np.array(augmented_targets_list,dtype=object)\n",
    "augmented_encoded_labels_array=augmented_labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe40c0-a390-4bce-a05c-e9b4a7b405ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_array=np.array(training_image_list)\n",
    "augmented_image_array=np.array(augmented_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338165a-eb92-412b-92bd-36f5be60cd2e",
   "metadata": {},
   "source": [
    "- We split the training data (90% for training and 10% for validation).\n",
    "- We stratify on the labels.\n",
    "- For reproducibility, we choose the seed that will be used in the training and validation splitting step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189af48-e94a-4607-87c6-c917a3012c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random_state=seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b829ff9-e005-47c0-89aa-789a3fcaa87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_array_train,Images_array_val,labels_array_train,labels_array_val=train_test_split(training_image_array,encoded_labels_array, test_size=0.1,stratify=encoded_labels_array[:,1],random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb7b0e-85a5-42d5-b3e9-bd1c83feea5f",
   "metadata": {},
   "source": [
    "- We split the augmented data (80% for training and 20% for validation).\n",
    "- We stratify on the augmented labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb145e-fab7-4a2d-9b57-01160f865885",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image_array_train,augmented_Images_array_val,augmented_encoded_labels_array_train,augmented_labels_array_val=train_test_split(augmented_image_array,augmented_encoded_labels_array, test_size=0.3,stratify=augmented_encoded_labels_array[:,1],random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ad67d-58f4-4930-8919-7805f89bfa39",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b7bc9-f010-4f4f-85fe-9539967a2862",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8620f-a76a-4d61-81c6-1bdf384274c9",
   "metadata": {},
   "source": [
    "- We create the function **load_training_data** to load the training and augmented training datasets using the class **Dental_disease_Dataset**.\n",
    "- We use the ConcatDataset tool to merge the training and augmented  datasets.\n",
    "- We create the training dataloader, that will put the data in the appropriate format with the defined batchsize and performing the shuffling of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4febec-27b1-41a5-a051-2ff00c6621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(batch_size,shuffle):\n",
    "    dataset_train_no_augmentation=Dental_disease_Dataset(image_list=Images_array_train,labels_list=labels_array_train,transformations=preprocessing)\n",
    "    augmented_dataset_train=Dental_disease_Dataset(image_list=augmented_image_array_train,labels_list=augmented_encoded_labels_array_train,transformations=augmented_preprocessing)\n",
    "    dataset_train=torch.utils.data.ConcatDataset([dataset_train_no_augmentation, augmented_dataset_train])\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle, drop_last=False,pin_memory=False,num_workers =0)\n",
    "    return data_loader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf1435-5109-4b9a-a42c-d8da29177516",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2791a60-6007-4ce3-9168-fad2fa1f49dc",
   "metadata": {},
   "source": [
    "- We create the function **load_validation_data** to load the validation and augmented validation datasets using the class **Dental_disease_Dataset**.\n",
    "- We use the ConcatDataset tool to merge the validation and augmented datasets.\n",
    "- We create the validation dataloader, that will put the data in the appropriate format with the defined batchsize and performing the shuffling of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d8aa3-b412-4375-8c1e-8aa60f064897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_data(batch_size,shuffle):\n",
    "    dataset_val_no_augmentation=Dental_disease_Dataset(image_list=Images_array_val,labels_list=labels_array_val,transformations=preprocessing)\n",
    "    augmented_dataset_val=Dental_disease_Dataset(image_list=augmented_Images_array_val,labels_list=augmented_labels_array_val,transformations=augmented_preprocessing)\n",
    "    dataset_val=torch.utils.data.ConcatDataset([dataset_val_no_augmentation, augmented_dataset_val])\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=shuffle, drop_last=False,pin_memory=False,num_workers =0)\n",
    "    return data_loader_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8fe83-558e-47dd-a37a-c7461e24a2d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Modelling parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fd447-4fa2-4f67-8b1d-a94989da8516",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choosing the model and the rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2a09a-ac1c-4ca6-86a2-11e816e5e55e",
   "metadata": {},
   "source": [
    "We have tested two state-of-the-art models: ResNet and EfficientNet. \n",
    "\n",
    "Resnet have been created by [He *et al*, 2015](https://arxiv.org/abs/1512.03385)   \n",
    "EfficientNet have been created by [Tan *et al*, 2019](https://arxiv.org/abs/1905.11946v5)   \n",
    "\n",
    "See the following for an usefull comparison of the two models, including their relative performances: https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html   \n",
    "\n",
    "After extensive testing between various models such as ResNet18, ResNet34, ResNet50 and all EfficientNet variants, we chose to use an EfficientNet for our model. \n",
    "\n",
    "EfficientNet is a convolutional neural network architecture that was introduced in 2019 by a group of researchers at Google. \n",
    "It follows a compound scaling approach, where the architecture is scaled in multiple dimensions simultaneously. The network's depth, width, and resolution are scaled together in a principled manner, resulting in a family of models labeled as EfficientNet-B0, EfficientNet-B1, EfficientNet-B2, and so on, with increasing model size and performance.\n",
    "\n",
    "The scaling process involves increasing the depth of the network by adding more layers, increasing the width of each layer by adding more channels, and increasing the resolution of the input images. This compound scaling strategy ensures that the network can effectively capture both low-level and high-level features, leading to improved performance.\n",
    "\n",
    "One key innovation in EfficientNet is the use of a compound scaling coefficient called \"phi\" (ϕ). This coefficient controls the overall network size, and different values of phi allow trade-offs between model size and accuracy. By varying phi, EfficientNet can be scaled up or down to suit different computational resources and task requirements.\n",
    "\n",
    "\n",
    "We chose EfficientNet v2 l after numerous attempts, as other models (B0 to B7) led to lower performances and other versions of EfficientNet v2 models such as v2s or m did not improve performances.\n",
    "\n",
    "The figure below shows the overal architecture of the EfficientNet B7 model.   \n",
    "We could not find an illustration for EfficientNet v2 but it follows the same overall architecture.\n",
    "\n",
    "<img src='Figures/effnet_b7.png' width=1000, height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35deb124-2376-402d-abfe-1cc4d4ab8e09",
   "metadata": {
    "tags": []
   },
   "source": [
    "For simplicity, we also choose to create four  **Fully Connected (FC) heads**, each predicting one of the required **class** (Cohort, AI type, Responsible Gene Name and Is_Syndromic_Isolated). This changes the nature of the problem, from a multi labels classification to a series of multi-class problems, which are intrinsically easier to compute.\n",
    "\n",
    "<img src='Figures/EfficientNet heads.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb12ce-a851-4488-b33d-fc1c869530aa",
   "metadata": {},
   "source": [
    "Below, we define the class **MultilabelClassifier** which replace the final fully connected head of an EfficientNet by four heads.   \n",
    "It takes as inputs:   \n",
    "- **model_type**: the type of model.\n",
    "- **in_features**: the associated number of features of the last convolutional layer, needed to define the number of layer in the four fully connected heads.\n",
    "- **n_cohort**: The number of labels in the Cohort class (three).\n",
    "- **n_AI_type**: The number of subtypes of Amelogenesis Imperfecta (five) in the AI_Type class.\n",
    "- **n_genes**: The number of responsible genes (six) in the Responsible_Gene_Name class. As explained above, we kept only MMP20, FAM83H,AMELX, FAM20A, ENAM and set all other to None.\n",
    "- **n_syndrome**: The number of labels in the Is_Isolated_Syndromic class (three)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e13c3-b4b7-49b5-83aa-46f6761a0764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultilabelClassifier(nn.Module):\n",
    "    def __init__(self,model_type,in_features, n_cohort, n_AI_type, n_genes, n_syndrome):\n",
    "        super().__init__()\n",
    "        self.model_wo_fc = nn.Sequential(*(list(model_type.children())[:-1]))\n",
    "\n",
    "        self.cohort =nn.Linear(in_features=in_features, out_features=n_cohort)    \n",
    "        self.AI_type = nn.Linear(in_features=in_features, out_features=n_AI_type)                  \n",
    "        self.genes = nn.Linear(in_features=in_features, out_features=n_genes)\n",
    "        self.syndrome = nn.Linear(in_features=in_features, out_features=n_syndrome) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return {\n",
    "            'Cohort': self.cohort(x),\n",
    "            'AI_Type': self.AI_type(x),\n",
    "            'Responsible_Gene_Name': self.genes(x),\n",
    "            'Is_Isolated_Syndromic': self.syndrome(x)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03caa8e8-4274-482b-a425-fea9beedbd9b",
   "metadata": {},
   "source": [
    "Each **head** will outputs **logits** for each possible labels in the class: \n",
    "\n",
    "- The head related to the **Cohort** class  outputs a vector of three logits, corresponding to the three possible labels (Normal, Amelogenesis Imperfecta, Dentinogeneis Imperfecta). \n",
    "- The head related to the **AI_Type class**  outputs a vector of five logits.\n",
    "- The head related to the **Responsible_Gene_Name** class  outputs a vector of six logits as we kept only five genes (MMP20, FAM83H, AMELX, FAM20A, ENAM) and the label None, and set all other genes to None.\n",
    "- The head related to the **Is_Isolated_Syndromic** class outputs a vector of three logits.\n",
    "\n",
    "\n",
    "<img src='Figures/EfficientNet Logits.png' width=800 height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709926f-a3c3-470e-b4f6-16bffe01b0d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "These logits can not be interpreted as **probabilities**, as their sum is not equal to one. Thus, to obtain the probability distribution, a **softmax** function will be applied to the logits, putting them in the range [0,1] with the sum being equal to one. This method allows to obtain the most probable label and the most unlikely labels of a particular class.\n",
    "\n",
    "<img src='Figures/SoftMax.png' width=800 height=200>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ec1f3-93ad-4f07-99e6-613422b3c0d0",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5abbb9-351a-40b7-98cb-7e594115d2da",
   "metadata": {
    "tags": []
   },
   "source": [
    "The loss function is an essential part of a deep learning model. It allows to measure the errors of the model by comparing the distance between the model's predictions and the ground truth.\n",
    "\n",
    "Minimizing the loss function will train the deep learning model to improve its performances, throught backpropagation and gradient descent. This means that the parameters of the model (weights and biases in every convolutional and fully connected layers) will be updated during the training  in order to minimize the loss function.\n",
    "\n",
    "One loss function commonly used in multi class classification is the cross entropy loss. In this challenge, we will compute four cross entropy losses, one for each of the head dedicated to a class (Cohort, AI type, Responsible Gene Name and Is_Syndromic_Isolated). Notice that in pytorch, the cross entropy loss function can take as input the raw logits from the model output, which do not need to be positive or having their sum  equal to one: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
    "\n",
    "\n",
    "\n",
    "<img src='Figures/Cross Entropy.png' width=1000 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429377b-1801-4491-a4e1-15dece769e1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The four losses will be averaged and the training objective will be to minimize the **global cross entropy** loss function. This will allow to adress the multi-label nature of the the problem, as we will obtain a predicted labels for each class of interest (Cohort, AI type, Responsible Gene Name and Is_Syndromic_Isolated).\n",
    "\n",
    "<img src='Figures/Global cross entropy.png' width=600 height=400>\n",
    "\n",
    "\n",
    "During our experiments, we noticed an imbalance in the importance of each individual loss value in the global loss. In order to correct for this and to favor models that predict well the classes **Cohort** and **AI_Type**; we added a coefficient for specific classes in the hyperparameter **class_coefficient**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e04c6-cebd-487d-b741-c4a61b747291",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=nn.CrossEntropyLoss(weight=None, reduction='mean')\n",
    "def criterion(loss_func,outputs,targets,class_coefficient):\n",
    "    losses = 0\n",
    "    for i, key in enumerate(outputs):\n",
    "        losses += class_coefficient[i]*loss_func(outputs[key].float(), targets[key].float().to(device))\n",
    "    return losses/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c47c6d-1603-4c98-8982-6c69e2d3b0d7",
   "metadata": {},
   "source": [
    "### Metric\n",
    "\n",
    "A criteria is required to choose the best model across all combinations of parameters we tried.    \n",
    "As the dataset is imbalanced, especially for the subtypes of Amelogenesis Imperfecta (Taurodontism and Hypomineralization) will use the F1 score, which takes into account not only the number of prediction errors made by the model, but that also looks at the type of errors that are made. \n",
    "\n",
    "F1 score is a combination of two metrics that also take into account class imbalance: \n",
    "- Precision: allow to reduce the number of False Positive in the model's predictions.\n",
    "- Recall: allow to reduce the number of False Negative in the model's predictions.\n",
    "\n",
    "This confusion matrix shows the true and false positives and the true and false negatives in the 2 class situation, along with the formulaes for the commonly used metrics. \n",
    "\n",
    "<img src='Figures/Confusion.png'>\n",
    "\n",
    "As we are interested in rare disease early detection in order to avoid diagnostic wandering, maximizing the recall is important in order to avoid missing patients with AI or DI.\n",
    "\n",
    "The computation of the F1 score allows for a generalization pictured above to any number of classes and any number of labels. \n",
    "The F1 score is the harmonic mean of precision and recall, both with equal weights. Thus, maximizing the F1 score means that the model's False Positives and False Negatives rates are minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e05bf-3f7e-4063-9d91-73353229365c",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Note that in scikit-learn, the F1 score cannot take the raw logits. We need to convert the model's output into one hot encoded format. We first apply a softmax on the raw logits, then using argmax (https://pytorch.org/docs/stable/generated/torch.argmax.html), we set the highest probability in each head to one, and the other to zero.\n",
    "\n",
    "<img src='Figures/Proba and Predictions.png' width=800 height=400>\n",
    "\n",
    "In this work, we will compute the **macro F1 score** for each of the four classes and then compute the **global F1 score** of the model's predictions, as we did for the cross entropy loss functions. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html. \n",
    "\n",
    "<img src='Figures/Global F1 score.png' width=800 height=400>\n",
    "\n",
    "More information on the different F1 score calculations can be found in the following: https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b49da0-9454-43f9-86cb-c5063f4b0a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Global_f1(outputs,targets):\n",
    "    Global_f1=0\n",
    "    for i, key in enumerate(outputs):\n",
    "        tar=targets[key].cpu().detach().numpy()\n",
    "        pred_soft=torch.nn.functional.softmax(outputs[key], dim=1)\n",
    "        for ind in range(pred_soft.shape[0]):\n",
    "            class_index=[torch.argmax(pred_soft[ind]).item()]\n",
    "            pred_soft[ind,class_index]=1\n",
    "        pred_soft[pred_soft<1]=0\n",
    "        pred_soft=pred_soft.cpu().detach().numpy()\n",
    "        Global_f1+=f1_score(pred_soft, tar, average='macro', zero_division=1)\n",
    "        \n",
    "    return Global_f1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08d610-5686-42bf-a382-0308f74ba88e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc87a7-6c8f-4085-b084-fc85d96b5e38",
   "metadata": {
    "tags": []
   },
   "source": [
    "The hyperparameters are a set of machine learning parameters whose values are chosen when the model is trained. We follow this [tutorial](https://cs230.stanford.edu/blog/hyperparameters/) from Dr. Andrew Ng to help us scan and choose the right values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba6813-6a25-4eff-8a53-d0a97f394806",
   "metadata": {},
   "source": [
    "We define the values for EfficientNet hypermarameters. These values were chosen after numerous test and gave the best performances of the model while keeping the training under 6 hours. \n",
    "\n",
    "We also added a coefficient 2 for the classes Cohort and AI_Type in the hyperparameter **class_coefficient**. The losses associated to the heads dedicated to the classes Cohort and AI_Type will be multiplied by two. This coefficient gave the best model (F1 score 0.72762)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88737dc0-9db6-49fa-ba43-22dad0f1a665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs=20\n",
    "lr=0.0001 \n",
    "batch_size=5\n",
    "batch_size_val_test=batch_size\n",
    "image_size=400\n",
    "\n",
    "class_coefficient=[2,2,1,1]\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fe8f6-92df-4d3f-a6dc-eb02c6a2dab5",
   "metadata": {},
   "source": [
    "We define the type of model and create appropriate names for the folders in which we will save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891657ef-f20b-4d05-9067-c209714e714f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name=\"efficientnet_v2_l\"\n",
    "pth_name=model_name+\"_\"+str(n_epochs)+\"epochs\"\n",
    "date=\"07_07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2dffe-77ed-434b-8152-178c87c84329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type=efficientnet_v2_l(weights='IMAGENET1K_V1')\n",
    "in_features=1280 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fae79f-15f9-4738-a49b-72ffc2d949d7",
   "metadata": {},
   "source": [
    "We defined the MultilabelClassifier function that will create the four classifiers to predict each of the four required elements. This step effectively changes the nature of the prediction problem from a multilabel classification to a set of multi-class problems. We then summarize the prediction of each multi-class model into a multi-label prediction. We believe that this is a more efficient and elegant way of addressing the challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043550ef-919e-4728-a8ac-ef9d1803e94e",
   "metadata": {},
   "source": [
    "We then call the MultilabelClassifier class with our choice of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a9644-c66c-4156-a3fd-e878b2cdfead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cohort=3 \n",
    "n_AI_type=5\n",
    "n_genes=6\n",
    "n_syndrome=3\n",
    "model=MultilabelClassifier(model_type,in_features, n_cohort, n_AI_type, n_genes, n_syndrome)\n",
    "\n",
    "model.float().to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcf308-41df-4582-ad88-37ebd2aea4a9",
   "metadata": {},
   "source": [
    "We use the optimizer function from Adam to make the learning rate evolve based on the scores obtained in the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcde1cb-96bf-432f-964d-0aad118f11b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e150f26-5a97-48b1-a478-2baf843f10a0",
   "metadata": {},
   "source": [
    "We create a folder to save the results, with the date and the combination of parameters we chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8822b5-b393-436f-a57e-3fffdc7b2d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_folder=date+\"/\"+str(n_genes)+\"genes_reso\"+str(image_size)+\"weightloss22_lr1e4_bis\"\n",
    "save_path = \"results/\"+result_folder+\"/\"+\"/\"+pth_name+\".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e7a21-d6d8-4072-89b8-c1ff52b24402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"results/\"+date+\"/\")==True:\n",
    "    pass\n",
    "elif os.path.exists(\"results/\"+date+\"/\")==False:\n",
    "    os.mkdir(\"results/\"+date+\"/\")\n",
    "if os.path.exists(\"results/\"+result_folder)==True:\n",
    "    pass\n",
    "elif os.path.exists(\"results/\"+result_folder)==False:\n",
    "    os.mkdir(\"results/\"+result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd56437-5915-423d-9834-e06302760680",
   "metadata": {},
   "source": [
    "Next step is to create a  dictionnary **hyperparam_dict** to store the model's hypermarameters and save it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc77f8-f8dd-4784-b928-d6b67ff81032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_dict={\"model\":model_name,\"n_genes\":n_genes,\"n_cohort\":n_cohort, \n",
    "\"n_AI_type\":n_AI_type,\n",
    "\"n_genes\":n_genes,\n",
    "\"n_syndrome\":n_syndrome,\"image_size\":image_size,\"n_epochs\":n_epochs,\"lr\":lr,\"batch_size\":batch_size,\"seed\":seed}\n",
    "with open(\"results/\"+result_folder+\"/hyperparam_dict.json\", \"w\") as fp:\n",
    "    json.dump(hyperparam_dict,fp,indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ac474-9908-4373-b205-675406dd3b8c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d63eec-b159-48a5-ba4d-3d2eebd5e79d",
   "metadata": {},
   "source": [
    "We execute the training and validation dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066b11e-25e7-4bf8-9806-581b26938fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle=True\n",
    "data_loader_train=load_training_data(batch_size,shuffle)\n",
    "data_loader_val=load_validation_data(batch_size,shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edc284-9d06-4960-841d-d71374868890",
   "metadata": {},
   "source": [
    "Below, we define the training of the network for a given number of epochs (n_epochs):\n",
    "- We start the training of the network and compute the global cross entropy loss and the global F1 score.\n",
    "- Then the validation starts, we compute the global cross entropy loss (without backpropagation) and the global F1 score. If the validation loss of the current epoch is less than the validation loss in the previous epoch, we save the model.\n",
    "- At the end of each epoch, we save the training/validation global loss and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa75ba-2bd5-4a9d-9277-dbcf7954ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list,val_loss_list=[], []\n",
    "f1_list,f1_val_list = [], []\n",
    "loss_per_class_list,val_loss_per_class_list=[], []\n",
    "f1_per_class_list,f1_val_per_class_list = [], []\n",
    "best_valid_loss,best_mean_valid_loss=5,5\n",
    "print(\"Start training\")\n",
    "start_time_sec = time()\n",
    "for epoch in range(n_epochs):\n",
    "    loss_epoch,loss_per_class_epoch,f1_epoch,f1_per_classes_epoch = [], [],[], []\n",
    "    train_loop=tqdm(data_loader_train)\n",
    "    model.train()\n",
    "    for images,targets in train_loop:\n",
    "        train_loop.set_description(f'Epoch {epoch+1}/{n_epochs}')\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred=model(images)\n",
    "  \n",
    "        losses = criterion(loss_func,pred, targets,class_coefficient)\n",
    "        f1_train=Global_f1(pred, targets)\n",
    "\n",
    "        losses.backward()       \n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch.append(losses.item())\n",
    "        f1_epoch.append(f1_train)\n",
    "    loss_epoch_mean = np.mean(np.array(loss_epoch))\n",
    "    f1_list.append(np.mean(f1_epoch))\n",
    "    loss_list.append(loss_epoch_mean) \n",
    "    print(\"Average loss for epoch = {:.4f} \".format(loss_epoch_mean))\n",
    "    print(\"Average  f1 for epoch = {:.4f} \".format(np.mean(f1_epoch)))\n",
    "\n",
    "    \n",
    "    loss_val_epoch,loss_val_per_class_epoch,f1_val_per_class_epoch,f1_val_epoch = [], [],[], []\n",
    "    val_loop=tqdm(data_loader_val)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images,targets in val_loop:\n",
    "            val_loop.set_description('valid')\n",
    "            images = images.to(device)\n",
    "            pred_val=model(images)\n",
    "\n",
    "            losses_val = criterion(loss_func,pred_val, targets,class_coefficient)\n",
    "            f1_val=Global_f1(pred_val, targets)\n",
    "\n",
    "            loss_val_epoch.append(losses_val.item())\n",
    "            f1_val_epoch.append(f1_val)\n",
    "    loss_val_epoch_mean = np.mean(loss_val_epoch) \n",
    "    # save best model\n",
    "    if loss_val_epoch_mean < best_mean_valid_loss:\n",
    "            best_mean_valid_loss = loss_val_epoch_mean\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"saved model\",pth_name)#, best_mean_valid_loss)\n",
    "            \n",
    "    val_loss_list.append(loss_val_epoch_mean) \n",
    "    f1_val_list.append(np.mean(f1_val_epoch))#np.mean\n",
    "\n",
    "    print(\"Average val loss for epoch = {:.4f} \".format(loss_val_epoch_mean))\n",
    "    print(\"Average val f1 for epoch = {:.4f} \".format(np.mean(f1_val_epoch)))\n",
    "\n",
    "    \n",
    "\n",
    "print('')\n",
    "end_time_sec       = time()\n",
    "total_time_sec     = end_time_sec - start_time_sec\n",
    "time_per_epoch_sec = total_time_sec / n_epochs\n",
    "\n",
    "print('Time per epoch: %5.2f minutes' % (time_per_epoch_sec/60))\n",
    "print('Total time : %5.2f minutes' % (total_time_sec/60),' %5.2f hours' % (total_time_sec/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5966c-dd34-4584-bc1f-14618876ea24",
   "metadata": {},
   "source": [
    "We plot and save the training and validation loss and F1 score as a function of the total number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad8a6a-ba78-453d-abe9-60f9db4bfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(n_epochs)), loss_list, label='traning')\n",
    "plt.plot(list(range(n_epochs)), val_loss_list, label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('results/'+result_folder+\"/\"+'Learning_Curves.png',format='png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(range(n_epochs)), f1_list, label='traning')\n",
    "plt.plot(list(range(n_epochs)), f1_val_list, label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.savefig('results/'+result_folder+\"/\"+'metrics.png',format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4bf8c-98f2-43a3-a124-7bd94708aab0",
   "metadata": {},
   "source": [
    "Below, we show the plots for the training and validation learning curves and metrics (F1 score) of our best model (F1 score 0.72762 on the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c5adf-b012-4d74-b359-09e71c1ea1f4",
   "metadata": {},
   "source": [
    "The validation loss follows closely the training loss, and increases a bit at the end. This indicates presence of overfitting. Lowering the learning rate to 0.0001 helped us to reduce overfitting. In fact, with higher learning rates, the validation loss was increasing while the training loss was decreasing after several epochs, leading to high overfitting and poorer F1 score on the validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45def47-d4fa-4a87-baff-f3dcc16bd1aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='6genes_reso400efficientnet_v2_lweightloss22_lr1e4_lr_step_bis/Learning_Curves.png' width=600 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d502f4-2968-4db4-b35e-7b26a4b0cbee",
   "metadata": {},
   "source": [
    "The validation F1 score closely follows the training F1 score, and decreases a bit at the end. This is expected and coherent with respect to the evolution of the losses. Notice that the training and validation F1 scores are high compared to the F1 score of 0.72762 on the test set, indicating overfitting. From all test tests we performed, we think more training data with more patients in the AI cohort are needed to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a0e58-5685-4985-a7df-0772efd94b10",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='6genes_reso400efficientnet_v2_lweightloss22_lr1e4_lr_step_bis/metrics.png' width=600 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1d1f2-6138-4909-9946-9cc97626ef0f",
   "metadata": {},
   "source": [
    "## Evaluation of the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902b710-87e4-4e3f-8b72-1367e58c9e43",
   "metadata": {},
   "source": [
    "### Loading the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c760fb-d1e2-4a9b-9298-504a9200371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path=\"data/test.csv\"\n",
    "image_id_path=\"data/images.csv\"\n",
    "patient_labels_list=pd.read_csv(test_path)[['trustii_id','Patient_ID']]\n",
    "image_id_list=pd.read_csv(image_id_path)[['Patient_ID','Image_ID','Patient_Age_In_Image','Image_Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d41478-a9e9-423b-9f62-9d713bfa6764",
   "metadata": {},
   "source": [
    "- We store the trustii id for all images of each patients in the list **test_trustii_id_list** \n",
    "- We store the images  in the list **test_image_list** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98604910-aaeb-41c5-9cd3-66cb1acd286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_list,test_trustii_id_list=[],[]\n",
    "pati_id=patient_labels_list['Patient_ID'].tolist()\n",
    "for pat_id in pati_id:\n",
    "    trustii_id=patient_labels_list[patient_labels_list['Patient_ID']==pat_id]['trustii_id'].item()\n",
    "    image_list=image_id_list[image_id_list['Patient_ID']==pat_id]['Image_ID'].tolist()\n",
    "    trustii_id_list=[trustii_id]*len(image_list)\n",
    "\n",
    "    for k in range(len(image_list)):\n",
    "        test_image_list.append(image_list[k])\n",
    "        test_trustii_id_list.append(trustii_id_list[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13143a1-b018-4473-9fc2-61dc8470f962",
   "metadata": {},
   "source": [
    "We apply the same pre processing to the images of the test set used on the training images, and store them in the list **final_image_list**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26856c90-1101-4048-bea6-e3e3f7047bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image_list=[]\n",
    "for image_id in tqdm(test_image_list):\n",
    "    loaded_image=cv2.imread(\"data/images/\"+image_id)\n",
    "    image=cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)\n",
    "    image = torch.tensor(image).permute(2, 0, 1).float()/255\n",
    "    image=preprocessing(image)\n",
    "\n",
    "    final_image_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701cd0c-d3a7-43db-9880-35a09ad314e2",
   "metadata": {},
   "source": [
    "We load the model that was saved with the lowest validation loss. Also, here we use the model that gave the best F1 score on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d31b6-68fc-497c-89cf-8459fac68df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"Final_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1e9d5-9efb-48a7-831c-949c84183008",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultilabelClassifier(model_type,in_features, n_cohort, n_AI_type, n_genes, n_syndrome)\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a350df-ad2c-4aa8-b927-a6ae471c1306",
   "metadata": {},
   "source": [
    "This time, we use the CPU to evaluate if the inference time if lower than a minute for one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00da66-f455-443a-bbc5-23f067926f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model.to(device).float()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e602c-c0f1-4e1c-84a3-7e7d31ee5f04",
   "metadata": {},
   "source": [
    "Using the four saved LabelBinarizer encoders, we decode the model prediction:\n",
    "\n",
    "\n",
    "<img src='Figures/Target decoding.png' width=800 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494102b-3bda-4a5a-8a8b-6251ee6b6870",
   "metadata": {},
   "source": [
    "Below, we use the saved model to and store the decoded predictions in the list **result_lists**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30488ef9-ef9f-4a81-aa8a-cfb9dd547a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_sec = time()\n",
    "\n",
    "enco_list=[]\n",
    "result_lists=[]\n",
    "model.eval()\n",
    "for im in tqdm(final_image_list):\n",
    "    im=im.unsqueeze(0).to(device)\n",
    "    prediction=model(im)\n",
    "    result={}\n",
    "    for i, key in enumerate(prediction):\n",
    "        pred_soft=torch.nn.functional.softmax(prediction[key], dim=1)\n",
    "        class_index=[torch.argmax(pred_soft).item()]\n",
    "        pred_soft[0,class_index]=1\n",
    "        pred_soft[pred_soft<1]=0\n",
    "        encoder=pickle.load(open(f'results/{key}_encoder', 'rb'))\n",
    "        result[key]=encoder.inverse_transform(np.array([pred_soft.cpu().detach().numpy()])[0]).tolist()[0]\n",
    "    result_lists.append(result)\n",
    "\n",
    "end_time_sec       = time()\n",
    "total_time_sec     = end_time_sec - start_time_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c2079-931f-4926-8023-86c024df490a",
   "metadata": {},
   "source": [
    "We compute the inference time on the CPU. Our model treats 757 images in less than nine minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f24d0-2ff7-4849-a356-1ef810519639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"took \"+str(total_time_sec/60)+\" minutes on CPU!\")\n",
    "print(\"took \"+str(total_time_sec/60/len(final_image_list))+\" per images minutes on CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9c562-73bd-431b-be97-5d83bbbf80bc",
   "metadata": {},
   "source": [
    "We create the dataframe **pred_test** that contains all the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94736a75-ab78-46b1-82f8-b954c2702d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=pd.DataFrame(result_lists)\n",
    "pred_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3bbfb-5d72-4a6f-b94c-26e1379cc833",
   "metadata": {},
   "source": [
    "We add the trustii id in the dataframe **pred_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e5457-5c06-4f4b-9fba-10388eb3bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test[\"trustii_id\"]=test_trustii_id_list\n",
    "pred_test.columns = ['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic','trustii_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fdf75-7887-44ed-8c0d-9d6798f91933",
   "metadata": {},
   "source": [
    "As several images may be available for a single patient, we use a majority vote to keep the prediction that occured the most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62883855-1d4b-4f19-b03a-d2ef9cd6c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_pred_test=pred_test.groupby('trustii_id').agg(lambda x : x.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298b1b6-4720-4193-b29b-5549bfba3550",
   "metadata": {},
   "source": [
    "We load the test.csv file and add the prediction to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2f9b7-eea0-4fdc-b7b2-c2a7ce8aebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv=pd.read_csv(test_path)\n",
    "test_csv[['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']]=patient_pred_test[['Cohort','AI_Type','Responsible_Gene_Name','Is_Isolated_Syndromic']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db247df0-9b27-41e0-b8e3-3af9c05b0044",
   "metadata": {},
   "source": [
    "We define the path and the name of the final csv file with the prediction.   \n",
    "We save the csv file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97927fbc-dda2-4cc9-a8fa-9aa7d9d84790",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_save_path=\"Final_submission.csv\"\n",
    "test_csv.to_csv(csv_save_path,index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb48586-ad5d-4a10-a1c8-08cb032cb96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model explainability definition\n",
    "- We use the package https://github.com/jacobgil/pytorch-grad-cam to visualize the regions where the model is looking at while predicting.\n",
    "- We consider the **CAM** (Class Activationn Map) proposed in Zhou *et al,*, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6808b7-e475-4371-8912-86dcb3a51d4e",
   "metadata": {},
   "source": [
    "- We define the target layers, in this case all the convolutional layers. Thus remove the fully connected heads, which correspond to model.model_wo_fc in the  MultilabelClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdb941-6a0e-491b-a98f-774eb1e5601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers=model.model_wo_fc\n",
    "cam = EigenCAM(model,target_layers, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73558c07-1cdf-4f07-9829-72b261ce587f",
   "metadata": {},
   "source": [
    "We choose several images with several prediction of AI_Type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9418922-1468-4071-a7c5-adb455f13832",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[100,200,500,650,700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4ae39-7d78-4662-b293-e7afeffb4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    print(index, result_lists[index][\"AI_Type\"])#Cohort\n",
    "    image=final_image_list[index].unsqueeze(0).to(device).float()\n",
    "    grayscale_cam = cam(image, targets=result_lists[index][\"AI_Type\"])\n",
    "    grayscale_cam_ = grayscale_cam[0, :,:]\n",
    "    image_float_np =np.array(image.squeeze(0).permute(1,2, 0).cpu().detach().numpy())\n",
    "    cam_image = show_cam_on_image(image_float_np, grayscale_cam_, use_rgb=True)\n",
    "    \n",
    "    fig=plt.figure(figsize=(40, 40))\n",
    "    axis1=fig.add_subplot(131)\n",
    "    axis1.axis('off')\n",
    "    axis1.imshow(image_float_np)\n",
    "    axis1.set_title(\"Predicted \"+result_lists[index][\"AI_Type\"],fontsize=30)\n",
    "\n",
    "    axis2=fig.add_subplot(132)\n",
    "    axis2.axis('off')\n",
    "    axis2.set_title('Class Activation Map',fontsize=20)\n",
    "    img_plot=axis2.imshow(cam_image,vmin = -1.2,vmax = 1.2)\n",
    "    fig.colorbar(img_plot, ax=axis2, location='right',shrink=0.3)\n",
    "    plt.savefig('results/'+result_folder+\"/\"+f'explain_{index}.png',format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f813d-b139-433c-ad58-504ebfeb9eb7",
   "metadata": {},
   "source": [
    "It is important to take into account the fact that as the model predicts not only the AI subtype but also the gene responsible and the syndromic nature, this affects the performance of the model and may affects the regions of interest in the images where the model is looking at while predicting. \n",
    "\n",
    "On the left, we display the original image from the test set with the model's **prediction** for the AI_Type class label, on the right the **Class Activation Map** and the associated color bar equivalent to the **SHAP values**.   \n",
    "Below, we show several images from our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd6de9-1776-4e30-86d5-3356623e3636",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='results/Best_results/23_06/4genes_reso400efficientnet_b7weightloss22_lr1e4/explain_100.png' width=800 height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e90b8-356a-4afb-9e03-c01b596a1252",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='results/Best_results/23_06/4genes_reso400efficientnet_b7weightloss22_lr1e4/explain_200.png' width=800 height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f448ce-9605-4839-8f94-a3e72af1e9d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='results/Best_results/23_06/4genes_reso400efficientnet_b7weightloss22_lr1e4/explain_500.png' width=800 height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f0212-32cc-4c51-bff2-52fdac5317b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='results/Best_results/23_06/4genes_reso400efficientnet_b7weightloss22_lr1e4/explain_650.png' width=800 height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3f598-045d-4237-b74d-d76b2edbe377",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='results/Best_results/23_06/4genes_reso400efficientnet_b7weightloss22_lr1e4/explain_700.png' width=800 height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5618f-7a33-412b-8e66-6917d717d9e3",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8177c-92a7-4fb9-94d4-a60c5f247ae9",
   "metadata": {},
   "source": [
    "Below, we define the function used to submit the csv file containing the prediction as well as the notebook used to train, evaluate the model and submit the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a629fec8-66a1-4b14-af69-75f9505ba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_submission_via_api(csv_file_path, ipynb_file_path, token, challenge_id):\n",
    "    # Create the API endpoint URL using the provided challenge ID\n",
    "    endpoint_url = f'https://api.trustii.io/api/ds/notebook/datasets/{challenge_id}/prediction'\n",
    "\n",
    "    # Read in the files as binary data\n",
    "    with open(csv_file_path, 'rb') as csv_file:\n",
    "        csv_file_data = csv_file.read()\n",
    "    with open(ipynb_file_path, 'rb') as ipynb_file:\n",
    "        ipynb_file_data = ipynb_file.read()\n",
    "\n",
    "    # Set up the request headers and data\n",
    "    headers = {'Trustii-Api-User-Token': token}\n",
    "\n",
    "    data = {\n",
    "        'csv_file': (csv_file_path, csv_file_data),\n",
    "        'ipynb_file': (ipynb_file_path, ipynb_file_data),\n",
    "    }\n",
    "\n",
    "    # Send the request to the API endpoint\n",
    "    response = requests.post(endpoint_url, headers=headers, files=data)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Return the response JSON\n",
    "        return json.loads(response.text)\n",
    "    else:\n",
    "        # If the request failed, raise an exception with the error message\n",
    "        raise Exception(f'Request failed with status code {response.status_code}: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a5633-81ce-4561-9dd2-c57c719a175d",
   "metadata": {},
   "source": [
    "Below, we set the path to the csv that gave the best result (F1 score 0.72762) for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905139fe-d56a-4473-a678-c13ab6586e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_submission_csv_path=\"Final_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57724dc2-a4a2-4040-9903-9f1c66479692",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path=best_submission_csv_path\n",
    "ipynb_file_path=\"Final_submission.ipynb\"\n",
    "challenge_id=1436"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb209e8-2fd4-4b83-a57c-a3c0aebccd9a",
   "metadata": {},
   "source": [
    "Notice that for long training (6 hours), the token is no more valid and has to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28cac03f-622a-40b9-99cd-0ceeac162696",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2ODkwMDMzODYsImVtYWlsIjoiYXNhcG9ydGFAZWlzYm0ub3JnIiwiZGF0YSI6eyJpZCI6MjExOTAsInVzZXJJZCI6OTI0LCJlbWFpbCI6ImFzYXBvcnRhQGVpc2JtLm9yZyJ9LCJpYXQiOjE2ODg5MTY5ODZ9.lBm4U4BTOD4uhO6fCWeb-cXQG8toAo_Q2UiC1a-E3AU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac98d06-6a97-45dd-bfb7-61fe64b70f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 7085,\n",
       "  'datasetId': 1436,\n",
       "  'vendorId': 737,\n",
       "  'modelId': None,\n",
       "  'creatorId': 924,\n",
       "  'privateScore': None,\n",
       "  'publicScore': {'accuracy_score': 0.9431045145330857,\n",
       "   'hamming_loss_score': 0.05689548546691404,\n",
       "   'precision_score': 0.9154422033024183,\n",
       "   'recall_score': 0.7265010609398921,\n",
       "   'f1_score': 0.7276233127825735,\n",
       "   'precision_score_micro': 0.9431045145330857,\n",
       "   'recall_score_micro': 0.9431045145330857,\n",
       "   'f1_score_micro': 0.9431045145330857},\n",
       "  'weight': 0,\n",
       "  'error': None,\n",
       "  'errorOrginal': None,\n",
       "  'name': 'Final_submission-1688918376348',\n",
       "  'displayName': 'Final_submission',\n",
       "  'status': 'validate',\n",
       "  'uploadedAt': '2023-07-09T15:59:36.470Z',\n",
       "  'defaultPublicScore': 0.7276233127825735,\n",
       "  'teamId': 587,\n",
       "  'selected': 'no',\n",
       "  'enterpriseId': 170,\n",
       "  'datasetName': 'patient_final-1680778362345',\n",
       "  'datasetDisplayName': 'Diagnodent challenge',\n",
       "  'mlProblem': 'multi-labels-classification',\n",
       "  'threshold': 0.5,\n",
       "  'endDate': '2023-07-09T21:59:00.000Z'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_submission_via_api(csv_file_path, ipynb_file_path, token, challenge_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae45200-a833-4043-84df-10fca2e0814f",
   "metadata": {},
   "source": [
    "## Comments and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735d65c-6537-4c41-ab81-27399195ab12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - General comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c3cca-6d4c-4d82-a4aa-5c40f8743403",
   "metadata": {},
   "source": [
    "We first would like to commend the organisers of this challenge for approaching this interesting medical problem in this way. We believe that this challenge is a great opportunity to breach the gap between data science, AI developpers and the medical/dentistry field. \n",
    "\n",
    "We also want to commend the trustii.io team for their reactivity and communication. \n",
    "\n",
    "It has long been part of our philosophy at the European Institute for Systems Biology and Medicine (EISBM) that the best results are achieved in a multidisciplinary environment with constant discussions and mutual improvement. Our team reflects this idea, being composed of biologists, engineers, physicists, mathematicians and developpers in artificial intelligence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b95c44-80a6-4370-9a11-a9808e60f015",
   "metadata": {},
   "source": [
    "### - Data and expectations comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0cf8bc-23a1-4da8-ad1c-5327ef6c286a",
   "metadata": {},
   "source": [
    "We have several comnments regarding the data made available and the expectations of the challenge. \n",
    "\n",
    "- It is very positive to have such good quality images repeated over time, although we are studying a rare condition.   \n",
    "- However, the data we have been given access to has been insufficient to achive all the goals of the challenge. As mentionned above, we cannot use the genetic information in most cases as the number of data points is too small to hope building an accurate and correctly fitted model.   \n",
    "- The way the data is encoded could be improved: as mentionned in the forum posts by another team, when the genetic information is coded to 'None', it can mean that either the test was not performed or that the test did not detect the presence of mutations in the known AI genes. Those two situations should have been coded differently as they carry a very different information content when building our models.\n",
    "- The comment above also applies to the 'Is_syndromic_isolated' column.  \n",
    "- Data scarcity has been an issue. It has proven to be very difficult to predict patients in the Type IV, as we only had access to 7 such records in the training set, which we even had to further stratify to create the training and validation split. Although this has been partially corrected by data augmentation, this is a suboptimal situation and we would recomment the organisers to give access to more records if available in order to better achieve the goals of the study. \n",
    "- It seems from the litterature that DI could be either syndromic or isolated (Andersson *et al*, 2018). However, in the training data we only have access to data from patients recorded as isolated. This needs further clarification but could impair the performances of our model if there are patients with a syndromic DI disease in the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a5a09-4e66-4dba-b098-2a01d8851379",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36efbe-9cbb-4b8d-a91e-af33d2653156",
   "metadata": {},
   "source": [
    "We have several ideas for future directions for the creation of a more accurate and interesting model and its implementation in the clinic that we would be happy to share if we win the challenge and start a collaboration on this topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f123a83-367e-4955-adbb-7f8d3d8ca1be",
   "metadata": {},
   "source": [
    "### - Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224608c-f40c-4469-b23d-9ffa24337f38",
   "metadata": {},
   "source": [
    "We believe we have created a competitive model, given the constraints and the available data. \n",
    "\n",
    "From our model, it seems it is relatively achieveable to predict the presence of a disease (AI, DI or none); the AI subtypes are harder to predict depending on the available data. The presence of genetic mutations, although well recognized in the litterature, seems  to not be predictable based on the imaging alone. It would be interesting to further explore this area with more detailed data. It would also be interesting to further explore the correlations between the gene mutations and the syndromic or isolated status of the disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0a5ab-aff4-4a38-82bf-297d9f811ec4",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4364c8-f4b1-46fb-80d9-41da319e969f",
   "metadata": {},
   "source": [
    "1. Sawan NM. Clear Aligners in Patients with Amelogenesis and Dentinogenesis Imperfecta. Int J Dent. 2021;2021:7343094.\n",
    "2. Januarti N, Ayu FV, Puspitawati R, Auerkari EI. Genetic and Epigenetic Aspects of Amelogenesis Imperfecta and Dentinogenesis Imperfecta. Atlantis Press; 2022. p. 435–43.\n",
    "3. Simancas-Escorcia VH, Natera-Guarapo AE, Camargo MGA. Genes involucrados en la Amelogénesis Imperfecta. Parte I. Revista Facultad de Odontología Universidad de Antioquia. 2018;30:105–20.\n",
    "4. Goldberg M. Genetic and structural alterations of enamel and dentin- amelogenesis imperfecta, dentinogenesis imperfecta and dentin dysplasia. Journal of Dental Health, Oral Disorders & Therapy. 2019;Volume 10 Issue 4.\n",
    "5. An C, Kumabe S, Hotta H, Mochida Y, Nakatsuka M, Ueda K, et al. Loss of FAM20A protein induces Amelogenesis Imperfecta in mice. https://www.oatext.com/pdf/IMM-2-142.pdf. Accessed 7 Jul 2023.\n",
    "6. Jaureguiberry G, De la Dure-Molla M, Parry D, Quentric M, Himmerkus N, Koike T, et al. Nephrocalcinosis (enamel renal syndrome) caused by autosomal recessive FAM20A mutations. Nephron Physiol. 2012;122:1–6.\n",
    "7. Witkop Jr. CJ. Amelogenesis imperfecta, dentinogenesis imperfecta and dentin dysplasia revisited: problems in classification. Journal of Oral Pathology & Medicine. 1988;17:547–53.\n",
    "8. Rarenet.eu. Amelogenesis Imperfecta - poster. http://www.rarenet.eu/wp-content/uploads/2013/12/Les-amelogeneses-imparfaites.pdf. Accessed 2 May 2023.\n",
    "9. de La Dure-Molla M, Philippe Fournier B, Berdal A. Isolated dentinogenesis imperfecta and dentin dysplasia: revision of the classification. Eur J Hum Genet. 2015;23:445–51.\n",
    "10. sklearn.preprocessing.LabelBinarizer. scikit-learn. https://scikit-learn/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html. Accessed 23 Jun 2023.\n",
    "11. ColorJitter — Torchvision main documentation. https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html. Accessed 5 Jun 2023.\n",
    "12. RandomHorizontalFlip — Torchvision 0.15 documentation. https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html. Accessed 23 Jun 2023.\n",
    "13. He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. 2015.\n",
    "14. Tan M, Le QV. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. 2020.\n",
    "15. EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling. 2019. https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html. Accessed 23 Jun 2023.\n",
    "16. CrossEntropyLoss — PyTorch 2.0 documentation. https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html. Accessed 5 Jun 2023.\n",
    "17. torch.argmax — PyTorch 2.0 documentation. https://pytorch.org/docs/stable/generated/torch.argmax.html. Accessed 23 Jun 2023.\n",
    "18. sklearn.metrics.f1_score. scikit-learn. https://scikit-learn/stable/modules/generated/sklearn.metrics.f1_score.html. Accessed 5 Jun 2023.\n",
    "19. Leung K. Micro, Macro & Weighted Averages of F1 Score, Clearly Explained. Medium. 2022. https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f. Accessed 6 Jun 2023.\n",
    "20. Logging and Hyperparameters. https://cs230.stanford.edu/blog/hyperparameters/. Accessed 5 Jun 2023.\n",
    "21. Gildenblat J. Advanced AI explainability for PyTorch. 2023.\n",
    "22. Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A. Learning Deep Features for Discriminative Localization. 2015.\n",
    "23. Andersson K, Malmgren B, Åström E, Dahllöf G. Dentinogenesis imperfecta type II in Swedish children and adolescents. Orphanet Journal of Rare Diseases. 2018;13:145."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
